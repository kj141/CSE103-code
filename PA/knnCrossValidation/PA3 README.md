# CSE103 Programming Assignment 3: k-NN Cross Validation #

Welcome to the third programming assignment of CSE103. **The due time is Wednesday, 11/20, 1 pm.**

This assignment uses K-NN algorithm to classify scanned images of handwritten digits. 
You will not be expected to implement the algorithm, but you will use cross validation to find the optimal number of neighbors (i.e. the k) for this task.

Setup
---------
You should all have your cs103f account set up. After you login to your cse103 account, in your home directory, create a new directory called pa3 and go into that directory:

        mkdir ~/pa3
        cd pa3

Copy all the files for this assignment from ../public/knnCrossValidation using
the following command. Notice the single dot at the end of this
command. The dot means the current working directory, which is pa3
after you executed the cd command above.

        cp /home/linux/ieng6/cs103f/public/knnCrossValidation/* .

  Files
----------

You need two files for this assignment: `digits.txt` and `knn_fit.py`.

- `digits.txt` contains the processed MNIST handwritten digit dataset.

The dataset has 1600 scanned images of size 28-by-28. Each image is a handwritten digit between 0 and 9. 

Each row in digits.txt represents an image. It has 785 integers. The first 784 integers form the feature vector of this image. The feature vector is generated by stacking the pixel intensities of the image. The pixel intensities are integers between 0 and 255. The last integer is a label that represents which digit (0-9) this image really is.

The dataset has approximately 160 images for each digit. In digits.txt, they are arranged in a random order.

- `knn_fit.py` is a python script (also an executable). It takes as input a training data file, a training label file, a testing data file and an integer k (number of neighbors). The output is a testing label file that contains the classification result.

These files are supposed to be in the same format as digits.txt (i.e. each row represents an image. For data files, each row contains 784 integers between 0 and 255; for label files, each row contains only the label).

As this script depends on numpy which is not installed on ieng6, for now we use a virtualenv. Run the following commands to activate the virtualenv:

    source /home/linux/ieng6/cs103f/public/virtualenv-1.10/myVE/bin/activate

After this, you will see (myVE) appear before the prompt. Then you can run the following command to see the options for this script.

    ./knn_fit.py --help
    
You can test the script with a sample training and testing dataset. Notice that the file after --test_label is the output. Open these sample txt files to see the format that this script expects the input files to be in.
    
    ./knn_fit.py --train_data sample_train_data.txt --train_label sample_train_label.txt --test_data sample_test_data.txt -k 3 --test_label sample_test_label.txt

 Task
----------

Your task in this assignment is using **M-fold cross-validation** to select the optimal **number of neighbors** (k).

The number of neighbors is an important parameter for k-NN algorithm. It is not always obvious what value of k to use before investigating the characteristics of the dataset. Cross validation is a technique that helps us choose such parameters in a disciplined way.

In class we have talked about **training error** and **testing error**. A parameter that gives small training error may lead to an over-fitting classifier. A classifier that overfits the training dataset often have difficulty predicting a completely new dataset (the test set) and thus gives large test error. Ultimately, test error (or generalization error) is what a good classifier should aim to reduce, not so much for the training error. Therefore in this task, we should choose k that gives the smallest test error.

How do we evaluate test error for a specific parameter, since in practice, we are often given a complete dataset without any training/testing partition? A straightforward method is to randomly split the dataset into two subsets, one for training and one for testing. A more sophisticated way is to do M-fold cross-validation.

In M-fold cross-validation, the original dataset is randomly partitioned into M subsets with roughly equal sizes. We then repeat the following process for M times:

- In each round, we select one subset as the test data, and use the remaining (M - 1) subsets as training set. Given this training/testing partition (also known as this "fold"), we run our classifier and obtain a test error rate. We call it the test error rate for this fold.

We use each subset as test data only once. The average test error rate over all folds gives an estimate of the generalization ability of the classifier using this specific k.

You will choose the number M (# of folds) according to your secret number (for looking up the scoresheet).
If your secret number falls between:

1000 - 3000, use M = 6;

3000 - 5000, use M = 7;

5000 - 7000, use M = 8;

7000 - 8000, use M = 9;

8000 - 9999, use M =10;

You are expected to do the following:
1. Split the dataset into M folds, prepare proper training and testing set for each fold.
2. Run K-NN executable for each fold and for each k in [3,5,7,9,11]. Based on classification result, compute test error in each case.
3. Compute the average test error for each k, then select the best k.

You will turn in a text file that contains your number M, a matrix of error rates and the optimal k you selected:
1. The first line of the file is the number M you are assigned according to your secret number.
2. Starting from the second line is a matrix of the shape (M+1) by 5 (because there are 5 choices for k). Fill the first M rows with test error rate for each fold and each k (with accuracy up to four decimal point). The last row will be the average test error over all folds for each k.
3. Then at the next line, print out the best k you choose.

Coding templates are provided in the assignment folder:
- For students familiar with or decide to write this assignment in Python, check out `knn_python_template.py`. You can directly use _knn_fit_ as function, rather than as executable.
- For students using other languages, check out `knn_nonpython_template.txt`.

Sample Output file:

6

0.0416  0.0335  0.1039  0.0020  0.0329

0.1685  0.1154  0.0179  0.1925  0.0774

0.1356  0.1343  0.1012  0.0230  0.0321

0.0248  0.0547  0.1942  0.1060  0.0206

0.0363  0.0393  0.1972  0.1532  0.0787

0.1524  0.1962  0.0951  0.0675  0.0227

0.0932  0.0955  0.1183  0.0907  0.0441

11


Submission
----------

After you generated the output file, run the following command to turn it in:

turnin -c cs103f [your output txt file name]

Congratulation! You are done.
